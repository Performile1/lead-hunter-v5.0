/**
 * Multi-LLM Orchestrator
 * Smart routing mellan olika LLM-providers baserat p√• uppgift och tillg√§nglighet
 */

import { analyzeWithGroq, isGroqAvailable } from './groqService';

export type LLMProvider = 'gemini' | 'groq' | 'openai' | 'claude' | 'ollama';

export interface LLMRequest {
  systemPrompt: string;
  userPrompt: string;
  temperature?: number;
  requiresWebSearch?: boolean;
  priority?: 'speed' | 'quality' | 'cost';
  maxTokens?: number;
}

export interface LLMResponse {
  text: string;
  provider: LLMProvider;
  cost?: number; // Uppskattad kostnad i USD
  duration?: number; // Millisekunder
  tokensUsed?: number;
  cached?: boolean;
}

export interface LLMStats {
  totalRequests: number;
  successfulRequests: number;
  failedRequests: number;
  totalCost: number;
  averageLatency: number;
  providerUsage: Record<LLMProvider, number>;
}

// Global statistik
const stats: LLMStats = {
  totalRequests: 0,
  successfulRequests: 0,
  failedRequests: 0,
  totalCost: 0,
  averageLatency: 0,
  providerUsage: {
    gemini: 0,
    groq: 0,
    openai: 0,
    claude: 0,
    ollama: 0
  }
};

/**
 * Smart LLM Router - v√§ljer b√§sta modell baserat p√• uppgift
 */
export async function analyzeSmart(request: LLMRequest): Promise<LLMResponse> {
  const startTime = Date.now();
  stats.totalRequests++;
  
  // V√§lj provider baserat p√• krav
  const provider = selectProvider(request);
  
  try {
    let text: string;
    let actualProvider = provider;
    
    switch (provider) {
      case 'groq':
        if (!isGroqAvailable()) {
          console.warn("Groq not available, falling back to Gemini");
          actualProvider = 'gemini';
          text = await analyzeWithGemini(request);
        } else {
          text = await analyzeWithGroq(
            request.systemPrompt,
            request.userPrompt,
            request.temperature || 0.2
          );
        }
        break;
        
      case 'gemini':
      default:
        text = await analyzeWithGemini(request);
        break;
    }
    
    const duration = Date.now() - startTime;
    const cost = estimateCost(actualProvider, text);
    
    // Uppdatera statistik
    stats.successfulRequests++;
    stats.totalCost += cost;
    stats.providerUsage[actualProvider]++;
    stats.averageLatency = (stats.averageLatency * (stats.successfulRequests - 1) + duration) / stats.successfulRequests;
    
    return {
      text,
      provider: actualProvider,
      duration,
      cost,
      tokensUsed: estimateTokens(text)
    };
    
  } catch (error: any) {
    stats.failedRequests++;
    console.error(`${provider} failed:`, error.message);
    
    // Fallback-kedja
    if (provider === 'gemini' && isGroqAvailable()) {
      console.log("Gemini failed, trying Groq fallback...");
      try {
        const text = await analyzeWithGroq(
          request.systemPrompt,
          request.userPrompt,
          request.temperature || 0.2
        );
        
        const duration = Date.now() - startTime;
        stats.successfulRequests++;
        stats.providerUsage.groq++;
        
        return {
          text,
          provider: 'groq',
          duration,
          cost: 0 // Groq √§r gratis
        };
      } catch (groqError) {
        console.error("Groq fallback also failed:", groqError);
      }
    }
    
    throw error;
  }
}

/**
 * Batch-analys med optimal provider-f√∂rdelning
 */
export async function analyzeBatch(
  requests: LLMRequest[],
  maxConcurrent: number = 5
): Promise<LLMResponse[]> {
  const results: LLMResponse[] = [];
  
  // Dela upp i batchar
  for (let i = 0; i < requests.length; i += maxConcurrent) {
    const batch = requests.slice(i, i + maxConcurrent);
    const batchResults = await Promise.all(
      batch.map(req => analyzeSmart(req))
    );
    results.push(...batchResults);
    
    // Liten paus mellan batchar f√∂r att undvika rate limits
    if (i + maxConcurrent < requests.length) {
      await new Promise(resolve => setTimeout(resolve, 1000));
    }
  }
  
  return results;
}

/**
 * V√§ljer b√§sta provider baserat p√• krav
 */
function selectProvider(request: LLMRequest): LLMProvider {
  // Om web search kr√§vs -> Gemini (har grounding)
  if (request.requiresWebSearch) {
    return 'gemini';
  }
  
  // Om hastighet prioriteras -> Groq (snabbast)
  if (request.priority === 'speed' && isGroqAvailable()) {
    return 'groq';
  }
  
  // Om kostnad prioriteras -> Groq (gratis)
  if (request.priority === 'cost' && isGroqAvailable()) {
    return 'groq';
  }
  
  // Om kvalitet prioriteras -> Gemini (bra balans)
  if (request.priority === 'quality') {
    return 'gemini';
  }
  
  // Default: Groq om tillg√§ngligt (gratis), annars Gemini
  return isGroqAvailable() ? 'groq' : 'gemini';
}

/**
 * Placeholder f√∂r Gemini-analys (anv√§nder befintlig geminiService)
 */
async function analyzeWithGemini(request: LLMRequest): Promise<string> {
  // Detta skulle anropa er befintliga generateWithRetry-funktion
  // F√∂r nu, kasta ett fel som indikerar att Gemini ska anv√§ndas
  throw new Error("GEMINI_REQUIRED");
}

/**
 * Uppskattar kostnad baserat p√• tokens
 */
function estimateCost(provider: LLMProvider, text: string): number {
  const tokens = estimateTokens(text);
  
  switch (provider) {
    case 'groq':
      return 0; // Gratis
    case 'openai':
      return (tokens / 1_000_000) * 0.60; // GPT-4o-mini output
    case 'gemini':
      return (tokens / 1_000_000) * 0.30; // Gemini Flash output
    case 'claude':
      return (tokens / 1_000_000) * 4.00; // Claude Haiku output
    default:
      return 0;
  }
}

/**
 * Uppskattar antal tokens (rough estimate)
 */
function estimateTokens(text: string): number {
  // Rough estimate: 1 token ‚âà 4 characters
  return Math.ceil(text.length / 4);
}

/**
 * H√§mtar statistik om LLM-anv√§ndning
 */
export function getLLMStats(): LLMStats {
  return { ...stats };
}

/**
 * √Öterst√§ller statistik
 */
export function resetLLMStats(): void {
  stats.totalRequests = 0;
  stats.successfulRequests = 0;
  stats.failedRequests = 0;
  stats.totalCost = 0;
  stats.averageLatency = 0;
  stats.providerUsage = {
    gemini: 0,
    groq: 0,
    openai: 0,
    claude: 0
  };
}

/**
 * Formaterar statistik till l√§sbar text
 */
export function formatLLMStats(): string {
  const successRate = stats.totalRequests > 0 
    ? ((stats.successfulRequests / stats.totalRequests) * 100).toFixed(1)
    : '0';
  
  return `
üìä LLM Statistik:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Totalt requests: ${stats.totalRequests}
Lyckade: ${stats.successfulRequests} (${successRate}%)
Misslyckade: ${stats.failedRequests}
Total kostnad: $${stats.totalCost.toFixed(2)}
Genomsnittlig latency: ${stats.averageLatency.toFixed(0)}ms

Provider-anv√§ndning:
  ‚Ä¢ Gemini: ${stats.providerUsage.gemini} (${((stats.providerUsage.gemini / stats.totalRequests) * 100).toFixed(0)}%)
  ‚Ä¢ Groq: ${stats.providerUsage.groq} (${((stats.providerUsage.groq / stats.totalRequests) * 100).toFixed(0)}%)
  ‚Ä¢ OpenAI: ${stats.providerUsage.openai} (${((stats.providerUsage.openai / stats.totalRequests) * 100).toFixed(0)}%)
  ‚Ä¢ Claude: ${stats.providerUsage.claude} (${((stats.providerUsage.claude / stats.totalRequests) * 100).toFixed(0)}%)
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
  `.trim();
}

/**
 * Rekommenderar b√§sta provider baserat p√• historisk prestanda
 */
export function recommendProvider(requiresWebSearch: boolean = false): LLMProvider {
  if (requiresWebSearch) return 'gemini';
  
  // Om Groq har h√∂g success rate och √§r tillg√§ngligt, rekommendera det
  if (isGroqAvailable() && stats.providerUsage.groq > 0) {
    return 'groq';
  }
  
  return 'gemini';
}
